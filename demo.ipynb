{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00b7cfd85d2fc43f76dbccc53202d8ba6a9a8cca408d693df58307e7c75a304a7",
   "display_name": "Python 3.8.8 64-bit ('atcs-practical': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0b7cfd85d2fc43f76dbccc53202d8ba6a9a8cca408d693df58307e7c75a304a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Demo for ACTS Assignments\n",
    "#### Rodrigo Alejandro Chavez Mulsa\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.Classifier import Classifier\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from modules.AverageEmbeddings import AverageEmbeddings\n",
    "from modules.Classifier import Classifier\n",
    "import torch\n",
    "from torchtext.legacy.datasets.nli import SNLI\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy import data\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "source": [
    "If you need the rest of the code to run it, it is available at: \n",
    "https://github.com/Noixas/ACTS-SNLI-Transfer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_path(model_name='awe'):\n",
    "    \"\"\"Get the path to the model checkpoints. Available: [awe, lstm, bilstm, bilstm-max]\n",
    "    \"\"\"\n",
    "    return'trained_models/'+model_name+'/gold/'+model_name+'.ckpt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(_premise,_hypothesis,model,print_premise = True, print_pred = True):\n",
    "    premise  = _premise.split(' ')\n",
    "    hypothesis  = _hypothesis.split(' ')\n",
    "\n",
    "    prem = TEXT.process([premise])\n",
    "    hyp = TEXT.process([hypothesis])\n",
    "\n",
    "    prem = [a.to(device) for a in prem] #Send to device\n",
    "    hyp = [a.to(device) for a in hyp] \n",
    "\n",
    "    pred = model.demo_inference(prem,hyp)\n",
    "    if print_premise:\n",
    "        print(\"Premise:\",_premise,'\\nHypothesis:',_hypothesis)\n",
    "    if print_pred:\n",
    "        print(\"Predicted label -->\",pred[1],'<-- with confidence:',pred[2])\n",
    "    return pred[-1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_models_prediction(_premise,_hypothesis,models_list):\n",
    "    print(\"Premise:\",_premise,'\\nHypothesis:',_hypothesis)\n",
    "    for model_name in models_list:        \n",
    "        print(\"Model:\",model_name)\n",
    "        model = load_model(model_name)\n",
    "        probs = get_prediction(_premise,_hypothesis,model,print_premise = False)\n",
    "        print(\"Probabilities\",probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    checkpath = get_checkpoint_path(model_name)\n",
    "    model = Classifier()\n",
    "    pretrained_model = model.load_from_checkpoint(checkpath,model_name=model_name,disable_nonlinear=True,emb_vec=TEXT.vocab.vectors).to(device)\n",
    "    return pretrained_model"
   ]
  },
  {
   "source": [
    "## Load models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(lower=True, include_lengths=True, batch_first=True,tokenize='spacy',tokenizer_language=\"en_core_web_sm\")\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "train, dev, test = SNLI.splits(TEXT, LABEL, root= './data')\n",
    "TEXT.build_vocab(train, vectors=glove)\n",
    "LABEL.build_vocab(train, specials_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "awe_model = load_model('awe')\n",
    "lstm_model = load_model('lstm')\n",
    "bilstm_model = load_model('bilstm')\n",
    "bilstm_max_model = load_model('bilstm-max')"
   ]
  },
  {
   "source": [
    "# Demo HERE\n",
    "Comment out the model to test and change the premise or hypothesis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twice the same cells in case we want to input some sentences but want to keep the basic example.\n",
    "#Entailment\n",
    "premise = \"Two woman are embracing while holding to go packages\"\n",
    "hypothesis = \"Two woman are holding packages\"\n",
    "#Contradiction\n",
    "premise=\"A man is typing on a machine used for stenography.\"\n",
    "hypothesis=\"The man is not operating a stenograph.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model = awe_model\n",
    "# active_model = lstm_model\n",
    "# active_model = bilstm_model\n",
    "active_model = bilstm_max_model"
   ]
  },
  {
   "source": [
    "#### Change input here!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No cat is outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Premise: A man is walking a dog \nHypothesis: No cat is outside\nPredicted label --> contradiction <-- with confidence: 0.9996337890625\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([7.8099544e-07, 9.9963379e-01, 3.6542348e-04], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "get_prediction(premise,hypothesis,active_model)"
   ]
  },
  {
   "source": [
    "## Scores \n",
    "In the following table we can see the accuracy scores for the NLI dev and test set along with the micro and macro scores of the transfer task which were measured by aggregating the accuracy scores of the following transfer tasks:\n",
    " \n",
    "`transfer_tasks=['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC','MRPC', 'SICKEntailment']`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Model      | NLI-dev | NLI-test | Transf-micro | Transf-macro |\n",
    "|------------|---------|----------|--------------|--------------|\n",
    "| AWE        | 0.6173  | 0.6283   | 82.573       | 79.129       |\n",
    "| LSTM       | 0.791   | 0.7834   | 79.894       | 78.337       |\n",
    "| BILSTM     | 0.7935  | 0.7948   | 83.36        | 82.185       |\n",
    "| BILSTM-MAX | 0.834   | 0.8333   | 87.075       | 84.95        |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the transfer tasks I used the suggested parameters to reproduce the results from the authors which are the following:    \n",
    "`params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}`    \n",
    "`params_senteval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,'tenacity': 5, 'epoch_size': 4}`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Error Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tokenization\n",
    "First would like to compare an example where the difference between the tokenization of `isn't` and `is not` give use different results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction with \"is not\" gives good results\n",
      "Expected: contradiction\n",
      "\n",
      "----------------------------------------------\n",
      "Premise: A man is typing on a machine used for stenography. \n",
      "Hypothesis: The man is not operating a stenograph.\n",
      "Model: awe\n",
      "Predicted label --> entailment <-- with confidence: 0.4668668806552887\n",
      "Probabilities [0.46686688 0.23888522 0.29424793]\n",
      "Model: lstm\n",
      "Predicted label --> entailment <-- with confidence: 0.7181154489517212\n",
      "Probabilities [0.71811545 0.22792126 0.0539633 ]\n",
      "Model: bilstm\n",
      "Predicted label --> contradiction <-- with confidence: 0.7327508330345154\n",
      "Probabilities [0.20750299 0.73275083 0.0597462 ]\n",
      "Model: bilstm-max\n",
      "Predicted label --> contradiction <-- with confidence: 0.8825724720954895\n",
      "Probabilities [0.06302392 0.8825725  0.05440364]\n",
      "----------------------------------------------\n",
      "Prediction with \"is not\" gives good results\n",
      "Expected: contradiction\n",
      "\n",
      "Premise: A man is typing on a machine used for stenography. \n",
      "Hypothesis: The man is'nt operating a stenograph.\n",
      "Model: awe\n",
      "Predicted label --> contradiction <-- with confidence: 0.7047304511070251\n",
      "Probabilities [0.05466229 0.70473045 0.24060722]\n",
      "Model: lstm\n",
      "Predicted label --> entailment <-- with confidence: 0.9009141325950623\n",
      "Probabilities [0.90091413 0.02650528 0.07258061]\n",
      "Model: bilstm\n",
      "Predicted label --> entailment <-- with confidence: 0.8888924717903137\n",
      "Probabilities [0.8888925  0.01987473 0.09123278]\n",
      "Model: bilstm-max\n",
      "Predicted label --> entailment <-- with confidence: 0.37128549814224243\n",
      "Probabilities [0.3712855  0.27347863 0.35523584]\n"
     ]
    }
   ],
   "source": [
    "premise=\"A man is typing on a machine used for stenography.\"\n",
    "print('Prediction with \"is not\" gives good results')\n",
    "print('Expected: contradiction\\n')\n",
    "\n",
    "print('----------------------------------------------')\n",
    "hypothesis=\"The man is not operating a stenograph.\"\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Prediction with \"is not\" gives good results')\n",
    "print('Expected: contradiction\\n')\n",
    "hypothesis=\"The man is'nt operating a stenograph.\"\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])"
   ]
  },
  {
   "source": [
    "Based on the previous scores we can see how depending on the tokenization of certain abbreviatures could bring completely different predictions. Spacy tokenizer transforms \"isn't\" to \"is\",\"n't\" and the glove vectors have a different vector for \"n't\" than \"not\". My hypothesis is that the amount of samples that use \"n't\" is less than the ones that use \"not\" so the models dont have as much data to learn with those examples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID of n't in glove: 40\nID of not in glove: 35\n"
     ]
    }
   ],
   "source": [
    "print(\"ID of n't in glove:\",glove.stoi[\"n't\"])\n",
    "print(\"ID of not in glove:\",glove.stoi[\"not\"])"
   ]
  },
  {
   "source": [
    "#### Examples from paper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In the analysis of the paper \"A large annotated corpus for learning natural language inference\", they mention an example with: \"A young girl throws sand toward the ocean\" as premise and \"A girl can’t stand the ocean\" as hypothesis and mention that all the models wrongly predict it as entailment, we can see in the cell bellow that is not the case for our models, they predict contradiction yet the expected label is neutral."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected: Neutral\n",
      "Premise: A young girl throws sand toward the ocean \n",
      "Hypothesis: A girl can’t stand the ocean\n",
      "Model: awe\n",
      "Predicted label --> neutral <-- with confidence: 0.5283992290496826\n",
      "Probabilities [0.02873656 0.44286418 0.5283992 ]\n",
      "Model: lstm\n",
      "Predicted label --> contradiction <-- with confidence: 0.4698620140552521\n",
      "Probabilities [0.43704367 0.469862   0.09309435]\n",
      "Model: bilstm\n",
      "Predicted label --> contradiction <-- with confidence: 0.49269798398017883\n",
      "Probabilities [0.4405183  0.49269798 0.06678371]\n",
      "Model: bilstm-max\n",
      "Predicted label --> contradiction <-- with confidence: 0.9215252995491028\n",
      "Probabilities [0.01286405 0.9215253  0.0656106 ]\n"
     ]
    }
   ],
   "source": [
    "premise = \"A young girl throws sand toward the ocean\"\n",
    "hypothesis = \"A girl can’t stand the ocean\"\n",
    "#neutral\n",
    "print('Expected: Neutral')\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])"
   ]
  },
  {
   "source": [
    "The lstm based models missclassifies the example as contradiction while the AWE Correclty classifies it as Neutral. We must pay attention to the probabilities where the lstm and bilstm have almost the same value for this example while the max pooling confidently predicts it a contradiction. The AWE correctly classifies the example but it does not mean it actually understand the context."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Contradiction misinterpreted as entailment\n",
    "Nevertheless the previous example does mean that our models outperform the models from the paper, we can see a similar example below where the models wrongly predict the example as entailment probably due to beach and ocean being associated but the models fail to understand the contradiction between fully clothed and naked. But the bilstm max pooling confidently predict it as contradiction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Premise: Three men stand on the beach, fully clothed. \n",
      "Hypothesis: Three naked men in the ocean.\n",
      "Model: awe\n",
      "Predicted label --> entailment <-- with confidence: 0.9874842762947083\n",
      "Probabilities [9.87484276e-01 6.79708493e-04 1.18359765e-02]\n",
      "Model: lstm\n",
      "Predicted label --> entailment <-- with confidence: 0.48467692732810974\n",
      "Probabilities [0.48467693 0.18359077 0.3317323 ]\n",
      "Model: bilstm\n",
      "Predicted label --> entailment <-- with confidence: 0.741700291633606\n",
      "Probabilities [0.7417003  0.11770273 0.14059706]\n",
      "Model: bilstm-max\n",
      "Predicted label --> contradiction <-- with confidence: 0.9837123155593872\n",
      "Probabilities [0.0012838  0.9837123  0.01500385]\n"
     ]
    }
   ],
   "source": [
    "premise = \"Three men stand on the beach, fully clothed.\"\n",
    "hypothesis = \"Three naked men in the ocean.\"\n",
    "#Contradiction\n",
    "print('Expected: Contradiction')\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])"
   ]
  },
  {
   "source": [
    "### Other examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected: Contradiction\n",
      "Premise: The kid is inside \n",
      "Hypothesis: The kid is outside\n",
      "Model: awe\n",
      "Predicted label --> entailment <-- with confidence: 0.999660849571228\n",
      "Probabilities [9.9966085e-01 6.5033839e-07 3.3850095e-04]\n",
      "Model: lstm\n",
      "Predicted label --> contradiction <-- with confidence: 0.693578839302063\n",
      "Probabilities [0.17099066 0.69357884 0.13543046]\n",
      "Model: bilstm\n",
      "Predicted label --> contradiction <-- with confidence: 0.7297667860984802\n",
      "Probabilities [0.0951359  0.7297668  0.17509724]\n",
      "Model: bilstm-max\n",
      "Predicted label --> contradiction <-- with confidence: 0.9805310368537903\n",
      "Probabilities [0.00177579 0.98053104 0.01769319]\n"
     ]
    }
   ],
   "source": [
    "premise = \"The kid is inside\"\n",
    "hypothesis = \"The kid is outside\"\n",
    "\n",
    "print('Expected: Contradiction')\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected: Contradiction\n",
      "Premise: Older lady checking out her goods at the check-out counter. \n",
      "Hypothesis: A woman is sitting on her bed.\n",
      "Model: awe\n",
      "Predicted label --> neutral <-- with confidence: 0.43991172313690186\n",
      "Probabilities [0.22904624 0.33104205 0.43991172]\n",
      "Model: lstm\n",
      "Predicted label --> entailment <-- with confidence: 0.8786997199058533\n",
      "Probabilities [0.8786997  0.00556939 0.11573087]\n",
      "Model: bilstm\n",
      "Predicted label --> entailment <-- with confidence: 0.8536057472229004\n",
      "Probabilities [0.85360575 0.02180326 0.12459102]\n",
      "Model: bilstm-max\n",
      "Predicted label --> contradiction <-- with confidence: 0.410432368516922\n",
      "Probabilities [0.40006053 0.41043237 0.18950711]\n"
     ]
    }
   ],
   "source": [
    "premise = \"Older lady checking out her goods at the check-out counter.\"\n",
    "hypothesis = \"A woman is sitting on her bed.\"\n",
    "#Contradiction\n",
    "print('Expected: Contradiction')\n",
    "get_all_models_prediction(premise,hypothesis,['awe','lstm','bilstm','bilstm-max'])"
   ]
  },
  {
   "source": [
    "### Dev data sanalysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f7e316b5ca0>>,\n",
       "            {'entailment': 0,\n",
       "             'contradiction': 1,\n",
       "             'neutral': 2,\n",
       "             '<unk>': 3,\n",
       "             \n",
       "             [torchtext.legacy.data.batch.Batch of size 64 from SNLI]\n",
       "             \t[.premise]:('[torch.cuda.LongTensor of size 64x27 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n",
       "             \t[.hypothesis]:('[torch.cuda.LongTensor of size 64x22 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n",
       "             \t[.label]:[torch.cuda.LongTensor of size 64 (GPU 0)]: 3})"
      ]
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_dev(model,name):\n",
    "    correct = 0\n",
    "    total = len(dev.examples)\n",
    "    correct_class = [0,0,0]\n",
    "    amount_class = [0,0,0]\n",
    "    from tqdm import tqdm\n",
    "    for i, ex in enumerate(tqdm(dev.examples)):\n",
    "        # print(\"\\nExpected Label\",ex.label)\n",
    "        pr = get_prediction(' '.join(ex.premise),' '.join(ex.hypothesis),model,False,print_pred=False)\n",
    "        prediction_n = np.argmax(pr)\n",
    "        correct_id_labe = LABEL.vocab.stoi[ex.label]\n",
    "        correct += 1 if correct_id_labe == prediction_n else 0\n",
    "        correct_class[correct_id_labe] += 1 if correct_id_labe == prediction_n else 0\n",
    "        amount_class[correct_id_labe] += 1 \n",
    "\n",
    "    acc = correct/total\n",
    "    acc_class = [correct_class[0]/amount_class[0], correct_class[1]/amount_class[1],correct_class[2]/amount_class[2]]\n",
    "    print(\"\\n \",name,\"Dev Accuracy:\",acc)\n",
    "    labels = ['entailment', 'contradiction', 'neutral']\n",
    "    print(labels)\n",
    "    print(\"Acc per class:\",acc_class)\n",
    "    return acc,acc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9842/9842 [00:04<00:00, 2346.03it/s]\n",
      "  0%|          | 14/9842 [00:00<01:13, 134.15it/s]\n",
      "  AWE Dev Accuracy: 0.6040438935175777\n",
      "['entailment', 'contradiction', 'neutral']\n",
      "Acc per class: [0.4373685791528988, 0.7535082367297132, 0.6241112828438949]\n",
      "100%|██████████| 9842/9842 [00:56<00:00, 173.62it/s]\n",
      "  0%|          | 8/9842 [00:00<02:08, 76.71it/s]\n",
      "  LSTM Dev Accuracy: 0.7958748221906117\n",
      "['entailment', 'contradiction', 'neutral']\n",
      "Acc per class: [0.8437969360168218, 0.8081147040878585, 0.7341576506955177]\n",
      "100%|██████████| 9842/9842 [01:44<00:00, 93.79it/s]\n",
      "  0%|          | 8/9842 [00:00<02:13, 73.47it/s]\n",
      "  BILSTM Dev Accuracy: 0.7897785003048161\n",
      "['entailment', 'contradiction', 'neutral']\n",
      "Acc per class: [0.8146590567738059, 0.801098230628432, 0.7527047913446677]\n",
      "100%|██████████| 9842/9842 [01:49<00:00, 89.61it/s]\n",
      "  BILSTM-MAX Dev Accuracy: 0.8292013818329608\n",
      "['entailment', 'contradiction', 'neutral']\n",
      "Acc per class: [0.878642234905377, 0.7928615009151921, 0.8151468315301391]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_awe  =get_acc_dev(awe_model,'AWE')\n",
    "acc_lstm  =get_acc_dev(lstm_model,'LSTM')\n",
    "acc_bilstm  =get_acc_dev(bilstm_model,'BILSTM')\n",
    "acc_bilstm_max  =get_acc_dev(bilstm_max_model,'BILSTM-MAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output in case notebook is cleaned\n",
    "# 100%|██████████| 9842/9842 [00:04<00:00, 2346.03it/s]\n",
    "#   0%|          | 14/9842 [00:00<01:13, 134.15it/s]\n",
    "#   AWE Dev Accuracy: 0.6040438935175777\n",
    "# ['entailment', 'contradiction', 'neutral']\n",
    "# Acc per class: [0.4373685791528988, 0.7535082367297132, 0.6241112828438949]\n",
    "# 100%|██████████| 9842/9842 [00:56<00:00, 173.62it/s]\n",
    "#   0%|          | 8/9842 [00:00<02:08, 76.71it/s]\n",
    "#   LSTM Dev Accuracy: 0.7958748221906117\n",
    "# ['entailment', 'contradiction', 'neutral']\n",
    "# Acc per class: [0.8437969360168218, 0.8081147040878585, 0.7341576506955177]\n",
    "# 100%|██████████| 9842/9842 [01:44<00:00, 93.79it/s]\n",
    "#   0%|          | 8/9842 [00:00<02:13, 73.47it/s]\n",
    "#   BILSTM Dev Accuracy: 0.7897785003048161\n",
    "# ['entailment', 'contradiction', 'neutral']\n",
    "# Acc per class: [0.8146590567738059, 0.801098230628432, 0.7527047913446677]\n",
    "# 100%|██████████| 9842/9842 [01:49<00:00, 89.61it/s]\n",
    "#   BILSTM-MAX Dev Accuracy: 0.8292013818329608\n",
    "# ['entailment', 'contradiction', 'neutral']\n",
    "# Acc per class: [0.878642234905377, 0.7928615009151921, 0.8151468315301391]"
   ]
  },
  {
   "source": [
    "On the output of the previous cell we can see that AWE model is better in finding contradiction but this could be simply that the model is biased to it and having certain words like \"not\" implies contradiciton.   \n",
    "In the LSTM based models the results are more balanced per class but neutral remains lower in the lstm and bilstm while for the max pooling model contradictions perform worse."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9842/9842 [00:04<00:00, 2271.92it/s]\n",
      "  AWE Dev Accuracy: 0.6040438935175777\n",
      "['entailment', 'contradiction', 'neutral']\n",
      "Acc per class: [0.4373685791528988, 0.7535082367297132, 0.6241112828438949]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_awe  =get_acc_dev(awe_model,'AWE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL.vocab.stoi"
   ]
  }
 ]
}